{
    "job_details": {
        "job_id": "4111826696",
        "job_title": "Data Engineer",
        "company": {
            "name": "Tata Consultancy Services",
            "location": "Mumbai, Maharashtra, India"
        },
        "job_description": {
            "overview": "Tata Consultancy Services is hiring Data Engineers with Alteryx and Snowflake !!!!",
            "details": "\nRole**Data Engineer with Alteryx and Snowflake\nExp**5-10\nLocation***Mumbai/Ahmedabad/Chennai\n\nKnowledge of:\n3 to 4 years of extensive hand-on experience in building data pipelines using Alteryx and Snowflake SQL procedural language\n2 to 3 years of experience in Airflow/MWAA service for data pipeline orchestration and schedule management\nExperience in Python – knowledge of various libraries to manage data extractions and transformations\nDeep knowledge and understanding of Snowflake Database Platform including features like external tables, clustering, various data ingestion techniques, Warehouse management etc.\nExpertise and excellent proficiency with Snowflake internals and integration of Snowflake with other technologies for data processing and reporting.\nKnowledge of Tableau is a plus.\nExceptional knowledge in writing complex SQLs and Procedure Language.\nAmazon Web Services like AWS Glue, Lambda, Secret Manager, S3, VPC, Private Link, Step Functions.\nExperience with Infrastructure as Code (IaC) tool like Terraform (preferred), AWS CloudFormation.\nShould have used GitHub repository for code management.\nExperience with any deployment tools like Jenkins.\nKnowledge of Data Modelling techniques.\nJIRA for project execution.\n\nResponsibilities include:\n\nPartnering with Business Users and other IT teams to design and develop Data Pipelines.\nSupport the production pipelines – operations and enhancements to existing pipelines.\nDoing impact analysis if any changes being requested upstream to the existing pipeline and data model.\nAbility to interpret the Business Requirements and convert them to technical requirements.\nProvide design considerations for scalability and reliability of data streams being ingested.\nImplementing ETL pipelines within and outside of a data warehouse using Python and Snowflake Snow SQL.\nKeeping the SOPs updated all the time for any changes.\nData profiling of various data sources to understand the relationships across them.\nDevelop optimized pipeline design to achieve acceptable performance.\nRecommend Data Quality Management methodologies for various projects.\nAssist in UAT phases of any new projects.\nFollow agile development framework to work any new projects.\nFollow Change Control management for any code deployment.\n\nSkill In:\n\nStrong interpersonal skills are required. Ability to work effectively with team members, clients, and other areas of the IT environment.\nStrong written and oral communications.\nExcellent problem-solving abilities, strong decision-making skills and make sound judgment decisions.\n\nInterested and eligible candidate can apply !!!!"
        }
    }
}